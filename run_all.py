import subprocess
import datetime
import os
import sys
import shutil
import json
import traceback
import yaml
import torch
import pandas as pd

def write_log(message, log_file):
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(message + "\n")

def run_command(command, log_file, section_name, cwd):
    write_log(f"\n===== ã€{section_name}ã€‘å¼€å§‹ ===== {datetime.datetime.now()}\n", log_file)
    try:
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True, cwd=cwd)
        if result.stdout:
            write_log(result.stdout, log_file)
        if result.stderr:
            write_log("\n[stderr]\n" + result.stderr, log_file)
        write_log(f"\n===== ã€{section_name}ã€‘ç»“æŸ ===== {datetime.datetime.now()}\n", log_file)
    except Exception as e:
        write_log(f"\n[EXCEPTION]: {e}\n{traceback.format_exc()}\n", log_file)

def get_device_info():
    info = {
        "Python Version": sys.version.split()[0],
        "Torch Version": torch.__version__,
        "CUDA Available": torch.cuda.is_available(),
        "CUDA Device": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
    }
    return info

def main():
    # æ ¹ç›®å½• results
    base_root = "results"
    os.makedirs(base_root, exist_ok=True)

    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    base_dir = os.path.join(base_root, timestamp)
    for d in ["checkpoints", "logs", "plots", "results"]:
        os.makedirs(os.path.join(base_dir, d), exist_ok=True)

    log_file = os.path.join(base_dir, "logs", "run.log")
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(f"========== å®éªŒæ—¥å¿—å¼€å§‹ [{datetime.datetime.now()}] ==========\n")

    # å¤åˆ¶å¹¶é‡å†™é…ç½®
    config_src = "configs/config.yaml"
    config_dst = os.path.join(base_dir, "config_copy.yaml")
    with open(config_src, "r") as f:
        cfg = yaml.safe_load(f)
    cfg["paths"]["save_dir"] = base_dir
    cfg["paths"]["scaler"] = os.path.join(base_dir, "scaler.pkl")
    with open(config_dst, "w") as f:
        yaml.dump(cfg, f, allow_unicode=True)

    write_log(f"å·²å¤åˆ¶å¹¶æ³¨å…¥é…ç½®åˆ° {config_dst}", log_file)
    write_log(f"è®¾å¤‡ä¿¡æ¯: {json.dumps(get_device_info(), ensure_ascii=False)}", log_file)

    # è®­ç»ƒ
    run_command(f"{sys.executable} train.py --config {config_dst}", log_file, "æ¨¡å‹è®­ç»ƒé˜¶æ®µ", cwd=os.getcwd())
    # é¢„æµ‹
    run_command(f"{sys.executable} predict.py --config {config_dst}", log_file, "é¢„æµ‹ä¸å¯è§†åŒ–é˜¶æ®µ", cwd=os.getcwd())

    # æ±‡æ€» summary
    summary = {
        "Experiment_ID": timestamp,
        "Start_Time": timestamp,
        "Config_File": config_dst,
        "Environment": get_device_info(),
        "Artifacts": {
            "Log_File": log_file,
            "Checkpoints": os.path.join(base_dir, "checkpoints"),
            "Plots": os.path.join(base_dir, "plots"),
            "Results": os.path.join(base_dir, "results")
        }
    }

    metrics_csv = os.path.join(base_dir, "plots", "training_metrics.csv")
    if os.path.exists(metrics_csv):
        try:
            df = pd.read_csv(metrics_csv)
            best_row = df.loc[df["ValLoss"].idxmin()]
            summary["Best_Validation_Loss"] = float(best_row["ValLoss"])
            summary["Final_Gating_Weights"] = {
                "Gas": float(best_row["Gate_Gas"]),
                "Liquid": float(best_row["Gate_Liquid"]),
                "Critical": float(best_row["Gate_Critical"]),
            }
        except Exception as e:
            summary["Metrics_Extraction_Error"] = str(e)

    summary_path = os.path.join(base_dir, "summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=4, ensure_ascii=False)

    write_log(f"\nâœ… å®éªŒå®Œæˆï¼Œæ‘˜è¦å·²ä¿å­˜è‡³ {summary_path}", log_file)
    write_log(f"ğŸ“ å®éªŒç›®å½•ï¼š{base_dir}", log_file)

if __name__ == "__main__":
    main()